{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "from sklearn import metrics\n",
    "from pipeline import create_pipeline\n",
    "import os\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 전처리\n",
    "# Get Dataset\n",
    "\n",
    "files={\n",
    "    'CICI':'/home/irteam/junghye-dcloud-dir/MLAC/new_data/CICI.csv',\n",
    "    'UNSW':'/home/irteam/junghye-dcloud-dir/MLAC/new_data/UNSW.csv'\n",
    "}\n",
    "\n",
    "data = pd.read_csv(files['CICI'])\n",
    "data=data[np.isfinite(data).all(1)]\n",
    "\n",
    "binary_target=data['label']\n",
    "\n",
    "data=data.drop(labels=['label','attack_category','nist_category'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Models\n",
    "models = []\n",
    "models.append(('RF', RandomForestClassifier(max_depth=5, n_estimators=5, max_features=3)))    \n",
    "models.append(('CART', DecisionTreeClassifier(max_depth=5)))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('QDA', QuadraticDiscriminantAnalysis()))\n",
    "models.append(('LR', LogisticRegression(solver='lbfgs', max_iter=200)))\n",
    "models.append(('ABoost', AdaBoostClassifier()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('MLP', MLPClassifier()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=['name','acc','f1_mi','f1_ma','f1_we','rc_mi','rc_ma','rc_we']+\\\n",
    "                 ['pc_mi','pc_ma','pc_we'])\n",
    "eval_path='/home/irteam/junghye-dcloud-dir/MLAC/evaluation'\n",
    "confusion_path='/home/irteam/junghye-dcloud-dir/MLAC/confusion_matrix/CICI_binary'\n",
    "cnt=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(data,binary_target,test_size=0.3, shuffle=True, stratify=binary_target, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix plot\n",
    "def plot_confusion_matrix(con_mat,labels,title:str,cmap=plt.cm.get_cmap('Blues'),normalize=False):\n",
    "    plt.imshow(con_mat,interpolation='nearest',cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    marks=np.arange(len(labels))\n",
    "    nlabels=[]\n",
    "    for k in range(len(con_mat)):\n",
    "        n=sum(con_mat[k])\n",
    "        nlabel='{0}(n={1})'.format(labels[k],n)\n",
    "        nlabels.append(nlabel)\n",
    "\n",
    "    plt.xticks(marks,labels)\n",
    "    plt.yticks(marks,nlabels)\n",
    "\n",
    "    thresh=con_mat.max()/2.\n",
    "    if normalize:\n",
    "        for i, j in itertools.product(range(con_mat.shape[0]), range(con_mat.shape[1])):\n",
    "            plt.text(j, i, '{0}%'.format(con_mat[i, j] * 100 / n), horizontalalignment=\"center\", color=\"white\" if con_mat[i, j] > thresh else \"black\")\n",
    "    else:\n",
    "        for i, j in itertools.product(range(con_mat.shape[0]), range(con_mat.shape[1])):\n",
    "            plt.text(j, i, con_mat[i, j], horizontalalignment=\"center\", color=\"white\" if con_mat[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    #plt.show()\n",
    "    #이미지 저장\n",
    "    plt.savefig(confusion_path+'/'+title+'.png',facecolor='#eeeeee')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:RF,acc:0.9707990845352286,f1_score:0.9707990845352286,0.9607043462713616,0.9703237425558238,recall:0.9707990845352286,0.9468811910272082,0.9707990845352286,precision:0.9707990845352286,0.9769821587948793,0.9713551966346159\n",
      "name:CART,acc:0.9612225565327313,f1_score:0.9612225565327313,0.9504394823883832,0.9616047011925579,recall:0.9612225565327313,0.9598379425409532,0.9612225565327313,precision:0.9612225565327313,0.9420557354817002,0.962630988023668\n",
      "name:NB,acc:0.7739617958810266,f1_score:0.7739617958810266,0.7563268743151864,0.7879874355499022,recall:0.7739617958810266,0.8399698145967354,0.7739617958810266,precision:0.7739617958810266,0.7614581898477408,0.8711396156195855\n",
      "name:LDA,acc:0.9625731915939194,f1_score:0.9625731915939194,0.9509122664180607,0.9624675609756216,recall:0.9625731915939194,0.948405421716106,0.9625731915939194,precision:0.9625731915939194,0.9534964724620768,0.9624102247120576\n",
      "name:QDA,acc:0.9650256605208135,f1_score:0.9650256605208135,0.9558009082775571,0.9655533205803937,recall:0.9650256605208135,0.9708461053814043,0.9650256605208135,precision:0.9650256605208135,0.9432405173744418,0.9676691072470998\n",
      "name:LR,acc:0.9743703861672717,f1_score:0.9743703861672717,0.966604276060192,0.9743824066226472,recall:0.9743703861672717,0.9670430705360866,0.9743703861672717,precision:0.9743703861672717,0.9661677060680555,0.9743958178954358\n",
      "name:ABoost,acc:0.9899598672854244,f1_score:0.9899598672854244,0.9868852651559097,0.9899521837239939,recall:0.9899598672854244,0.9861430752760174,0.9899598672854244,precision:0.9899598672854244,0.9876335110586395,0.989948268943953\n"
     ]
    }
   ],
   "source": [
    "for name, model in models:\n",
    "    #data_loader()\n",
    "    # binary classification\n",
    "    binary_model=create_pipeline(model)\n",
    "   \n",
    "    print('training start...')\n",
    "    binary_model.fit(X_train,y_train)\n",
    "    \n",
    "    #evaluation\n",
    "    print('evaluation start...')\n",
    "    binary_pred=binary_model.predict(X_test)\n",
    "    #evaluation result\n",
    "    model_eval=[]\n",
    "    model_eval.append(name)\n",
    "    \n",
    "    acc = accuracy_score(y_test, binary_pred)\n",
    "    f1_mi = f1_score(y_test, binary_pred,average='micro')\n",
    "    f1_ma = f1_score(y_test, binary_pred,average='macro')\n",
    "    f1_we = f1_score(y_test, binary_pred,average='weighted')\n",
    "    recall_mi = recall_score(y_test, binary_pred, average='micro')\n",
    "    recall_ma = recall_score(y_test, binary_pred, average='macro')\n",
    "    recall_we = recall_score(y_test, binary_pred, average='weighted')\n",
    "    precision_mi = precision_score(y_test, binary_pred, average='micro')\n",
    "    precision_ma = precision_score(y_test, binary_pred, average='macro')\n",
    "    precision_we = precision_score(y_test, binary_pred, average='weighted')\n",
    "    \n",
    "    model_eval.extend([acc,f1_mi,f1_ma,f1_we,recall_mi,recall_ma,recall_we,precision_mi,precision_ma,precision_we])\n",
    "    \n",
    "    #confusion_metrics\n",
    "    confusion=metrics.confusion_matrix(y_test,binary_pred)\n",
    "    plot_confusion_matrix(confusion,labels=list(set(binary_target)),title=name)\n",
    "       \n",
    "\n",
    "\n",
    "    print(f'name:{name},acc:{acc},f1_score:{f1_mi},{f1_ma},{f1_we},recall:{recall_mi},{recall_ma},{recall_we},precision:{precision_mi},{precision_ma},{precision_we}')\n",
    "    df.loc[cnt]=model_eval\n",
    "\n",
    "    cnt+=1\n",
    "    \n",
    "\n",
    "df.to_csv(os.path.join(eval_path,'CICI_binary.csv'),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 굳이..\n",
    "from PIL import Image\n",
    "\n",
    "#파일명\n",
    "img_files=os.listdir(confusion_path)\n",
    "\n",
    "images=[] # 이미지 리스트\n",
    "for img_file in img_files:\n",
    "    img=Image.open(img_file)\n",
    "    img.resize((400,300))\n",
    "    images.append(img)\n",
    "\n",
    "# 크기\n",
    "width,height=images[0].size\n",
    "new_width=width*3\n",
    "new_height=height*3\n",
    "new_img=Image.new('RGB',(new_width,new_height))\n",
    "\n",
    "#이미지 합치기\n",
    "x_offset=0\n",
    "y_offset=0\n",
    "for img in images:\n",
    "    new_img.paste(img,(x_offset,y_offset))\n",
    "    x_offset+=width\n",
    "    if x_offset==new_width:\n",
    "        x_offset=0\n",
    "        y_offset+=height\n",
    "\n",
    "\n",
    "new_img.save(confusion_path+'merged_image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
