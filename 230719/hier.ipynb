{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cici_test_dir='/home/irteam/junghye-dcloud-dir/MLAC/230624/CICI'\n",
    "X_test=pd.read_csv(os.path.join(cici_test_dir,'X_test.csv'))\n",
    "y_test=pd.read_csv(os.path.join(cici_test_dir,'y_test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.DataFrame(columns=['L1_acc','L1_f1','L1_rc','L1_pc','L2_acc','L2_f1','L2_rc','L2_pc']+\\\n",
    "                 ['c1_acc','c1_f1','c1_rc','c1_pc']+\\\n",
    "                    ['c2_acc','c2_f1','c2_rc','c2_pc','c3_acc','c3_f1','c3_rc','c3_pc','c4_acc','c4_f1','c4_rc','c4_pc']+\\\n",
    "                     ['total_acc','total_f1','total_rc','total_pc'])\n",
    "\n",
    "\n",
    "cnt=0\n",
    "model_eval=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "L1_ytest=y_test.copy()\n",
    "\n",
    "L2_ytest=X_test['nist_category'].copy()\n",
    "\n",
    "L3_ytest=X_test['attack_category'].copy()\n",
    "\n",
    "c1_Xtest=X_test.query('nist_category==1')\n",
    "c1_ytest=c1_Xtest['attack_category']\n",
    "\n",
    "c2_Xtest=X_test.query('nist_category==2')\n",
    "c2_ytest=c2_Xtest['attack_category']\n",
    "\n",
    "c3_Xtest=X_test.query('nist_category==3')\n",
    "c3_ytest=c3_Xtest['attack_category']\n",
    "\n",
    "c4_Xtest=X_test.query('nist_category==4')\n",
    "c4_ytest=c4_Xtest['attack_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_data in [c1_Xtest,c2_Xtest,c3_Xtest,c4_Xtest]:\n",
    "    class_data.drop(labels=['attack_category','nist_category'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.drop(labels=['Unnamed: 0','nist_category','attack_category'],axis=1,inplace=True)\n",
    "L1_ytest.drop(labels=['Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_result(model:str,test,pred) ->list:\n",
    "\n",
    "    acc=accuracy_score(test,pred)\n",
    "    f1=f1_score(test,pred,average='weighted')\n",
    "    recall=recall_score(test,pred,average='weighted')\n",
    "    precision=precision_score(test,pred,average='weighted')\n",
    "\n",
    "    result=[acc,f1,recall,precision]\n",
    "    result=[round(num,3) for num in result]\n",
    "\n",
    "    print(f'{model} result , acc:{result[0]}, f1:{result[1]},recall:{result[2]},precision:{result[3]}')\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_path='/home/irteam/dcloud-global-dir/MLAC/saved_models/230620/savedmodels'\n",
    "confusion_path='/home/irteam/junghye-dcloud-dir/MLAC/230719/result/CICI/confusion'\n",
    "out_txt_path='/home/irteam/junghye-dcloud-dir/MLAC/230719/result/CICI/CICI_v1.txt'\n",
    "outpath='/home/irteam/junghye-dcloud-dir/MLAC/230719/result/CICI/CICI_v1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(con_mat,labels,title:str,cmap=plt.cm.get_cmap('Blues'),normalize=False):\n",
    "    plt.figure(figsize=(20,15))\n",
    "    plt.imshow(con_mat,interpolation='nearest',cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    marks=np.arange(len(labels))\n",
    "    nlabels=[]\n",
    "    for k in range(len(con_mat)):\n",
    "        n=sum(con_mat[k])\n",
    "        nlabel='{0}(n={1})'.format(labels[k],n)\n",
    "        nlabels.append(nlabel)\n",
    "\n",
    "    plt.xticks(marks,labels,rotation=45)\n",
    "    plt.yticks(marks,nlabels)\n",
    "\n",
    "    thresh=con_mat.max()/2.\n",
    "    if normalize:\n",
    "        for i, j in itertools.product(range(con_mat.shape[0]), range(con_mat.shape[1])):\n",
    "            plt.text(j, i, '{0}%'.format(con_mat[i, j] * 100 / n), horizontalalignment=\"center\", color=\"white\" if con_mat[i, j] > thresh else \"black\")\n",
    "    else:\n",
    "        for i, j in itertools.product(range(con_mat.shape[0]), range(con_mat.shape[1])):\n",
    "            plt.text(j, i, con_mat[i, j], horizontalalignment=\"center\", color=\"white\" if con_mat[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    #plt.show()\n",
    "    #이미지 저장\n",
    "    plt.savefig(confusion_path+'/'+title+'.png',facecolor='#eeeeee')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "L1_model=joblib.load(os.path.join(saved_path,'CICI_L1mod_rc.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 test starts...\n",
      "result is acc, recall, time\n",
      "epoch:1, 0.992, 0.992, 242.468\n",
      "epoch:2, 0.99, 0.99, 234.125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate output file\n",
    "if not os.path.isfile(out_txt_path):\n",
    "    Path(out_txt_path).touch()\n",
    "\n",
    "# csv writer\n",
    "f=open(out_txt_path,'w')\n",
    "f.truncate()\n",
    "\n",
    "\n",
    "batch_size=1000\n",
    "\n",
    "chunk_size=X_test.shape[0]//batch_size\n",
    "remainder=X_test.shape[0]%batch_size\n",
    "\n",
    "start_index=0\n",
    "end_index=0\n",
    "L1_ypred=[]\n",
    "print('L1 test starts...')\n",
    "\n",
    "print('result is acc, recall, time')\n",
    "\n",
    "for i in range(0,X_test.shape[0],chunk_size):\n",
    "    f=open(out_txt_path,'a')\n",
    "    start_time=time.time()\n",
    "\n",
    "    # end_index\n",
    "    end_index=start_index+chunk_size\n",
    "\n",
    "    # remainder 처리 \n",
    "    if start_index+chunk_size>X_test.shape[0]:\n",
    "        end_index=start_index+remainder\n",
    "\n",
    "\n",
    "    y_pred=L1_model.predict(X_test.iloc[start_index:end_index])\n",
    "    y_test=L1_ytest.iloc[start_index:end_index]\n",
    "    \n",
    "    acc=accuracy_score(y_test,y_pred)\n",
    "    recall=recall_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "\n",
    "    print('Batch:{:}, {:}, {:}, {:}'.format(i//chunk_size+1,round(acc,3),round(recall,3),round(time.time()-start_time,3)))\n",
    "    # 중간 결과 저장\n",
    "    f.write('Batch:{:}, {:}, {:}, {:}'.format(i//chunk_size+1,round(acc,3),round(recall,3),round(time.time()-start_time,3))+'\\n')\n",
    "    f.close()\n",
    "\n",
    "    # 예측값 합치기\n",
    "    L1_ypred.extend(y_pred)\n",
    "    \n",
    "    # start_index update\n",
    "    start_index=end_index\n",
    "\n",
    "# Layer1 result\n",
    "eval_result=test_result(L1_model,L1_ytest,L1_ypred)\n",
    "model_eval.extend(eval_result)\n",
    "\n",
    "# L1 결과 저장 \n",
    "f.write('L1 result'+','.join(map(str,eval_result))+'\\n')\n",
    "f2=open('/home/irteam/junghye-dcloud-dir/MLAC/230719/result/CICI/CICI_L1.txt','w')\n",
    "f2.truncate()\n",
    "f2.write(','.join(map(str,L1_ypred)))\n",
    "f2.close()\n",
    "\n",
    "#confusion\n",
    "confusion=metrics.confusion_matrix(L1_ytest,L1_ypred)\n",
    "plot_confusion_matrix(confusion,labels=[0,1],title='Layer1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_indices=np.where(L1_ypred==1)[0]\n",
    "\n",
    "# 최종 결과를 위해 저장\n",
    "L1_b_ypred=[L1_ypred[i] for i in range(len(L1_ypred)) if i not in malicious_indices]\n",
    "L1_b_ytest=[L1_ytest[i] for i in range(len(L1_ytest)) if i not in malicious_indices]\n",
    "\n",
    "\n",
    "if malicious_indices.any():\n",
    "    L2_model=joblib.load(os.path.join(saved_path,'CICI_nist.pkl'))\n",
    "    L2_Xtest=X_test.iloc[malicious_indices]\n",
    "    L2_ypred=L2_model.predict(L2_Xtest)\n",
    "    L2_ytest_selected=L2_ytest.iloc[malicious_indices]\n",
    "    L2_result=test_result(L2_model,L2_ytest_selected,L2_ypred)\n",
    "    \n",
    "    # Layer2 result\n",
    "    f=open(out_txt_path,'w')\n",
    "    f.write('L2 result'+','.join(map(str,L2_result))+'\\n')\n",
    "    f.close()\n",
    "    model_eval.extend(L2_result)\n",
    "else:\n",
    "    print('no malicious predicted')\n",
    "    import sys\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "L2_encoded=[]\n",
    "L2_encoded.extend(L2_ypred)\n",
    "L2_encoded.extend(L2_ytest_selected)\n",
    "L2_encoded=list(set(L2_encoded))\n",
    "\n",
    "confusion=metrics.confusion_matrix(L2_ytest_selected,L2_ypred)\n",
    "plot_confusion_matrix(confusion,labels=L2_encoded,title='Layer2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class_models = [\n",
    "    joblib.load(os.path.join(saved_path, 'class_1_CICI.pkl')),\n",
    "    joblib.load(os.path.join(saved_path, 'class_2_CICI.pkl')),\n",
    "    joblib.load(os.path.join(saved_path, 'class_3_CICI.pkl')),\n",
    "    joblib.load(os.path.join(saved_path, 'class_4_CICI.pkl'))\n",
    "]\n",
    "\n",
    "class_names = ['Reconnaissance', 'Access', 'Dos', 'Malware']\n",
    "\n",
    "class_encodings=defaultdict(list)\n",
    "final_y_pred=[]\n",
    "final_y_test=[]\n",
    "\n",
    "L3_ytest_selected=L3_ytest.iloc[malicious_indices]\n",
    "\n",
    "for class_index,class_model in enumerate(class_models):\n",
    "    indices=np.where(L2_ypred==class_index+1)[0]\n",
    "    print(class_names[class_index]+'train & test')\n",
    "\n",
    "    if indices.any():\n",
    "        X_test_selected=L2_Xtest.iloc[indices]\n",
    "        y_pred=class_model.predict(X_test_selected)\n",
    "        y_test_selected=L3_ytest_selected.iloc[indices]\n",
    "        result=test_result(class_model,y_test_selected,y_pred)\n",
    "\n",
    "        # 각 classifier result\n",
    "        f=open(out_txt_path,'w')\n",
    "        f.write(str(class_names[class_index])+','.join(map(str,result))+'\\n')\n",
    "        f.close()\n",
    "\n",
    "        model_eval.extend(result)\n",
    "\n",
    "        class_encodings[class_names[class_index]].extend(y_pred)\n",
    "        class_encodings[class_names[class_index]].extend(y_test_selected)\n",
    "\n",
    "        final_y_pred.extend(y_pred)\n",
    "        final_y_test.extend(y_test_selected)\n",
    "        \n",
    "\n",
    "    else:\n",
    "        model_eval.extend([0,0,0,0])\n",
    "\n",
    "\n",
    "final_y_pred.extend(L1_b_ypred)\n",
    "final_y_test.extend(L1_b_ytest)\n",
    "\n",
    "\n",
    "final_result = test_result('Layer3', final_y_test, final_y_pred)\n",
    "\n",
    "# final result\n",
    "f=open(out_txt_path,'w')\n",
    "f.write('final result'+','.join(map(str,final_result))+'\\n')\n",
    "f.close()\n",
    "\n",
    "model_eval.extend(final_result)\n",
    "\n",
    "for class_name in class_names:\n",
    "    confusion = metrics.confusion_matrix(class_encodings[class_name][len(class_encodings[class_name]) // 2:], \n",
    "                                         class_encodings[class_name][:len(class_encodings[class_name]) // 2])\n",
    "    plot_confusion_matrix(confusion,labels=list(set(class_encodings[class_name])),title=class_name)\n",
    "\n",
    "confusion = metrics.confusion_matrix(final_y_test, final_y_pred)\n",
    "plot_confusion_matrix(confusion, labels=list(set(final_y_pred + final_y_test)), title='Layer3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[cnt]=model_eval\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(outpath))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
